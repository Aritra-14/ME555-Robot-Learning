{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Author: Aritra Ray**\n",
        "Description: Using Pix2PixHD Model for Next Frame in Sequence Prediction, on my own video! \n",
        "\n",
        "\n",
        "Details of Pix2PixHD: https://github.com/NVIDIA/pix2pixHD"
      ],
      "metadata": {
        "id": "-B2sCmDYYUWN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfOexYWJX3Pt"
      },
      "source": [
        "## Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K--AsrIzpH58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4440fba8-457d-4511-cf83-63773925ae32"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9D_7iUQINoa"
      },
      "source": [
        "Check to see what GPU we’re assigned"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABG3hL4lIQGP",
        "outputId": "405477b7-9c2f-4c00-b775-fbf12b7f262c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!nvidia-smi -L"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 0: A100-SXM4-40GB (UUID: GPU-a1f469b5-b936-6e94-1420-91d5bab9318b)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUvLbCJtLqaV"
      },
      "source": [
        "## Install libraries and dependencies\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQbRsmWdvjUo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ad5fb3a-5831-4ffb-e7e9-376cb3facddc"
      },
      "source": [
        "import os\n",
        "if os.path.isdir(\"/content/drive/MyDrive/nfp-colab/pix2pixHD/\"):\n",
        "    %cd /content/drive/MyDrive/nfp-colab/pix2pixHD/\n",
        "    # !git pull\n",
        "    !pip install dominate\n",
        "else:\n",
        "    %cd /content/drive/MyDrive\n",
        "    !mkdir nfp-colab\n",
        "    %cd nfp-colab\n",
        "    !git clone -b video https://github.com/dvschultz/pix2pixHD.git\n",
        "    !pip install dominate\n",
        "    %cd pix2pixHD/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/nfp-colab/pix2pixHD\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting dominate\n",
            "  Downloading dominate-2.7.0-py2.py3-none-any.whl (29 kB)\n",
            "Installing collected packages: dominate\n",
            "Successfully installed dominate-2.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzLGf05WXMFV"
      },
      "source": [
        "## Extract frames from video\n",
        "\n",
        "Upload a video to either Colab or Google Drive.\n",
        "\n",
        "* `-video` is the path to the video file\n",
        "* `-name` should be a unique name (think of it like a project name)\n",
        "* `-width` and `-height` **must** to be a multiple of 32\n",
        "* `-p2pdir` leave as `.` unless you know it shouldn’t be that ;)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWRL2ty6N9LD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bab6f06a-9748-4f61-9e97-031535a19132"
      },
      "source": [
        "!python extract_frames.py -video /content/aritra-ray.mp4 -name aritra-ray -p2pdir . -width 1280 -height 736"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "creating the dataset structure\n",
            "ffmpeg -v 16 -i /content/aritra-ray.mp4 -q:v 2 -vf \"scale=iw*736/ih:736, crop=1280:736\" /content/drive/MyDrive/nfp-colab/pix2pixHD/datasets/aritra-ray/train_frames/frame-%06d.jpg -hide_banner\n",
            "extracting the frames\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qL9BZkBA_QRR"
      },
      "source": [
        "## Train your model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4X7qahzMX05u"
      },
      "source": [
        "### Initial training\n",
        "\n",
        "Note: if you have a large dataset, this may timeout initially (`ValueError: __len__() should return >= 0`). Give it a minute or two and run it again.\n",
        "\n",
        "*   `--name` should be a unique name (think of it like a project name). For your sanity I recommend using the same `-name` as above.\n",
        "*   `--dataroot` should point to your dataset. This will usually be in `./datasets/[your project name]`\n",
        "\n",
        " \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzHBGVnUKEzE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7c90267-86d9-4380-adaf-7b93df2ec08c"
      },
      "source": [
        "!python train_video.py --name aritra-ray --dataroot ./datasets/aritra-ray/ --save_epoch_freq 5 #--continue_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------ Options -------------\n",
            "batchSize: 1\n",
            "beta1: 0.5\n",
            "checkpoints_dir: ./checkpoints\n",
            "continue_train: False\n",
            "data_type: 32\n",
            "dataroot: ./datasets/aritra-ray/\n",
            "debug: False\n",
            "display_freq: 100\n",
            "display_winsize: 512\n",
            "feat_num: 3\n",
            "fineSize: 512\n",
            "fp16: False\n",
            "fps: 24.0\n",
            "gpu_ids: [0]\n",
            "heat_seeking_lvl: 0\n",
            "input_nc: 3\n",
            "instance_feat: False\n",
            "isTrain: True\n",
            "label_feat: False\n",
            "label_nc: 35\n",
            "lambda_feat: 10.0\n",
            "loadSize: 1024\n",
            "load_features: False\n",
            "load_pretrain: \n",
            "local_rank: 0\n",
            "lr: 0.0002\n",
            "max_dataset_size: inf\n",
            "model: pix2pixHD\n",
            "nThreads: 2\n",
            "n_blocks_global: 9\n",
            "n_blocks_local: 3\n",
            "n_clusters: 10\n",
            "n_downsample_E: 4\n",
            "n_downsample_global: 4\n",
            "n_layers_D: 3\n",
            "n_local_enhancers: 1\n",
            "name: aritra-ray\n",
            "ndf: 64\n",
            "nef: 16\n",
            "netG: global\n",
            "ngf: 64\n",
            "niter: 100\n",
            "niter_decay: 100\n",
            "niter_fix_global: 0\n",
            "no_flip: False\n",
            "no_ganFeat_loss: False\n",
            "no_html: False\n",
            "no_instance: False\n",
            "no_lsgan: False\n",
            "no_vgg_loss: False\n",
            "norm: instance\n",
            "num_D: 2\n",
            "output_nc: 3\n",
            "phase: train\n",
            "pool_size: 0\n",
            "print_freq: 100\n",
            "pstart: 1\n",
            "pstop: 1\n",
            "resize_or_crop: scale_width\n",
            "save_epoch_freq: 5\n",
            "save_latest_freq: 1000\n",
            "scheduled_sampling: False\n",
            "serial_batches: False\n",
            "ss_recursion_prob: 0.2\n",
            "start_frames_before: 1\n",
            "start_from: video\n",
            "tf_log: False\n",
            "use_dropout: False\n",
            "verbose: False\n",
            "video_mode: False\n",
            "which_epoch: latest\n",
            "zoom_cres: False\n",
            "zoom_inc: 24\n",
            "zoom_lvl: 0\n",
            "-------------- End ----------------\n",
            "train_video.py:11: DeprecationWarning: fractions.gcd() is deprecated. Use math.gcd() instead.\n",
            "  def lcm(a,b): return abs(a * b)/fractions.gcd(a,b) if a and b else 0\n",
            "CustomDatasetDataLoader\n",
            "dataset [FrameDataset] was created\n",
            "FrameDataset initialized from: ./datasets/aritra-ray/train_frames\n",
            "contains 125 frames, 124 consecutive pairs\n",
            "#training images = 124\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n",
            "100% 548M/548M [00:01<00:00, 305MB/s]\n",
            "create web directory ./checkpoints/aritra-ray/web...\n",
            "(epoch: 1, iters: 100, time: 0.282) G_GAN: 0.712 G_GAN_Feat: 2.589 G_VGG: 3.486 D_real: 0.498 D_fake: 0.435 \n",
            "End of epoch 1 / 200 \t Time Taken: 33 sec\n",
            "(epoch: 2, iters: 76, time: 0.204) G_GAN: 1.056 G_GAN_Feat: 3.931 G_VGG: 2.961 D_real: 0.479 D_fake: 0.496 \n",
            "End of epoch 2 / 200 \t Time Taken: 25 sec\n",
            "(epoch: 3, iters: 52, time: 0.204) G_GAN: 0.787 G_GAN_Feat: 2.428 G_VGG: 2.753 D_real: 0.417 D_fake: 0.413 \n",
            "End of epoch 3 / 200 \t Time Taken: 25 sec\n",
            "(epoch: 4, iters: 28, time: 0.203) G_GAN: 1.127 G_GAN_Feat: 4.298 G_VGG: 2.568 D_real: 0.116 D_fake: 0.266 \n",
            "End of epoch 4 / 200 \t Time Taken: 25 sec\n",
            "(epoch: 5, iters: 4, time: 0.203) G_GAN: 1.964 G_GAN_Feat: 4.617 G_VGG: 2.582 D_real: 0.409 D_fake: 0.214 \n",
            "(epoch: 5, iters: 104, time: 0.201) G_GAN: 0.974 G_GAN_Feat: 3.009 G_VGG: 2.411 D_real: 0.412 D_fake: 0.681 \n",
            "End of epoch 5 / 200 \t Time Taken: 25 sec\n",
            "saving the model at the end of epoch 5, iters 620\n",
            "(epoch: 6, iters: 80, time: 0.374) G_GAN: 1.060 G_GAN_Feat: 3.523 G_VGG: 2.515 D_real: 0.118 D_fake: 0.296 \n",
            "End of epoch 6 / 200 \t Time Taken: 25 sec\n",
            "(epoch: 7, iters: 56, time: 0.204) G_GAN: 2.643 G_GAN_Feat: 6.091 G_VGG: 2.539 D_real: 0.243 D_fake: 0.177 \n",
            "End of epoch 7 / 200 \t Time Taken: 25 sec\n",
            "(epoch: 8, iters: 32, time: 0.204) G_GAN: 1.452 G_GAN_Feat: 3.989 G_VGG: 2.587 D_real: 0.236 D_fake: 0.154 \n",
            "End of epoch 8 / 200 \t Time Taken: 25 sec\n",
            "(epoch: 9, iters: 8, time: 0.204) G_GAN: 0.794 G_GAN_Feat: 3.031 G_VGG: 2.261 D_real: 0.370 D_fake: 0.432 \n",
            "saving the latest model (epoch 9, total_steps 1000)\n",
            "(epoch: 9, iters: 108, time: 0.201) G_GAN: 1.422 G_GAN_Feat: 3.717 G_VGG: 2.180 D_real: 0.487 D_fake: 0.154 \n",
            "End of epoch 9 / 200 \t Time Taken: 28 sec\n",
            "(epoch: 10, iters: 84, time: 0.204) G_GAN: 1.730 G_GAN_Feat: 4.708 G_VGG: 2.157 D_real: 0.083 D_fake: 0.046 \n",
            "End of epoch 10 / 200 \t Time Taken: 25 sec\n",
            "saving the model at the end of epoch 10, iters 1240\n",
            "(epoch: 11, iters: 60, time: 0.256) G_GAN: 2.505 G_GAN_Feat: 3.847 G_VGG: 2.326 D_real: 0.676 D_fake: 0.200 \n",
            "End of epoch 11 / 200 \t Time Taken: 25 sec\n",
            "(epoch: 12, iters: 36, time: 0.204) G_GAN: 2.326 G_GAN_Feat: 5.496 G_VGG: 2.211 D_real: 0.064 D_fake: 0.043 \n",
            "End of epoch 12 / 200 \t Time Taken: 25 sec\n",
            "(epoch: 13, iters: 12, time: 0.204) G_GAN: 0.503 G_GAN_Feat: 3.545 G_VGG: 2.271 D_real: 0.126 D_fake: 0.547 \n",
            "(epoch: 13, iters: 112, time: 0.201) G_GAN: 1.719 G_GAN_Feat: 3.930 G_VGG: 2.043 D_real: 0.078 D_fake: 0.053 \n",
            "End of epoch 13 / 200 \t Time Taken: 25 sec\n",
            "(epoch: 14, iters: 88, time: 0.204) G_GAN: 1.221 G_GAN_Feat: 4.387 G_VGG: 2.162 D_real: 0.104 D_fake: 0.178 \n",
            "End of epoch 14 / 200 \t Time Taken: 25 sec\n",
            "(epoch: 15, iters: 64, time: 0.204) G_GAN: 2.201 G_GAN_Feat: 5.102 G_VGG: 2.255 D_real: 0.036 D_fake: 0.058 \n",
            "End of epoch 15 / 200 \t Time Taken: 25 sec\n",
            "saving the model at the end of epoch 15, iters 1860\n",
            "(epoch: 16, iters: 40, time: 0.256) G_GAN: 2.256 G_GAN_Feat: 3.981 G_VGG: 2.161 D_real: 0.130 D_fake: 0.104 \n",
            "End of epoch 16 / 200 \t Time Taken: 25 sec\n",
            "(epoch: 17, iters: 16, time: 0.204) G_GAN: 1.572 G_GAN_Feat: 3.847 G_VGG: 2.063 D_real: 0.225 D_fake: 0.241 \n",
            "saving the latest model (epoch 17, total_steps 2000)\n",
            "(epoch: 17, iters: 116, time: 0.201) G_GAN: 2.435 G_GAN_Feat: 6.634 G_VGG: 2.495 D_real: 0.062 D_fake: 0.034 \n",
            "End of epoch 17 / 200 \t Time Taken: 28 sec\n",
            "(epoch: 18, iters: 92, time: 0.204) G_GAN: 2.083 G_GAN_Feat: 5.354 G_VGG: 1.952 D_real: 0.774 D_fake: 0.843 \n",
            "End of epoch 18 / 200 \t Time Taken: 25 sec\n",
            "(epoch: 19, iters: 68, time: 0.204) G_GAN: 1.624 G_GAN_Feat: 4.624 G_VGG: 2.018 D_real: 0.065 D_fake: 0.067 \n",
            "End of epoch 19 / 200 \t Time Taken: 25 sec\n",
            "(epoch: 20, iters: 44, time: 0.204) G_GAN: 1.827 G_GAN_Feat: 4.289 G_VGG: 2.108 D_real: 0.283 D_fake: 0.122 \n",
            "End of epoch 20 / 200 \t Time Taken: 25 sec\n",
            "saving the model at the end of epoch 20, iters 2480\n",
            "(epoch: 21, iters: 20, time: 0.258) G_GAN: 1.945 G_GAN_Feat: 4.099 G_VGG: 2.001 D_real: 0.289 D_fake: 0.177 \n",
            "(epoch: 21, iters: 120, time: 0.201) G_GAN: 1.825 G_GAN_Feat: 5.017 G_VGG: 2.047 D_real: 0.107 D_fake: 0.057 \n",
            "End of epoch 21 / 200 \t Time Taken: 25 sec\n",
            "(epoch: 22, iters: 96, time: 0.204) G_GAN: 1.257 G_GAN_Feat: 3.105 G_VGG: 1.938 D_real: 0.410 D_fake: 0.333 \n",
            "End of epoch 22 / 200 \t Time Taken: 25 sec\n",
            "(epoch: 23, iters: 72, time: 0.204) G_GAN: 1.253 G_GAN_Feat: 3.414 G_VGG: 1.990 D_real: 0.555 D_fake: 0.141 \n",
            "End of epoch 23 / 200 \t Time Taken: 25 sec\n",
            "(epoch: 24, iters: 48, time: 0.204) G_GAN: 2.652 G_GAN_Feat: 5.198 G_VGG: 2.069 D_real: 0.553 D_fake: 0.147 \n",
            "End of epoch 24 / 200 \t Time Taken: 25 sec\n",
            "(epoch: 25, iters: 24, time: 0.204) G_GAN: 1.253 G_GAN_Feat: 5.800 G_VGG: 1.999 D_real: 0.092 D_fake: 0.393 \n",
            "saving the latest model (epoch 25, total_steps 3000)\n",
            "(epoch: 25, iters: 124, time: 0.201) G_GAN: 3.242 G_GAN_Feat: 5.857 G_VGG: 2.283 D_real: 0.259 D_fake: 0.202 \n",
            "End of epoch 25 / 200 \t Time Taken: 28 sec\n",
            "saving the model at the end of epoch 25, iters 3100\n",
            "(epoch: 26, iters: 100, time: 0.201) G_GAN: 2.126 G_GAN_Feat: 4.629 G_VGG: 1.939 D_real: 0.214 D_fake: 0.057 \n",
            "End of epoch 26 / 200 \t Time Taken: 25 sec\n",
            "(epoch: 27, iters: 76, time: 0.204) G_GAN: 1.634 G_GAN_Feat: 5.344 G_VGG: 1.862 D_real: 0.048 D_fake: 0.039 \n",
            "End of epoch 27 / 200 \t Time Taken: 25 sec\n",
            "(epoch: 28, iters: 52, time: 0.204) G_GAN: 2.856 G_GAN_Feat: 6.531 G_VGG: 2.157 D_real: 0.119 D_fake: 0.150 \n",
            "End of epoch 28 / 200 \t Time Taken: 25 sec\n",
            "(epoch: 29, iters: 28, time: 0.204) G_GAN: 2.125 G_GAN_Feat: 4.821 G_VGG: 1.944 D_real: 0.473 D_fake: 0.301 \n",
            "End of epoch 29 / 200 \t Time Taken: 25 sec\n",
            "(epoch: 30, iters: 4, time: 0.204) G_GAN: 2.837 G_GAN_Feat: 6.683 G_VGG: 1.935 D_real: 0.029 D_fake: 0.096 \n",
            "(epoch: 30, iters: 104, time: 0.201) G_GAN: 0.417 G_GAN_Feat: 3.082 G_VGG: 2.235 D_real: 0.264 D_fake: 0.882 \n",
            "End of epoch 30 / 200 \t Time Taken: 25 sec\n",
            "saving the model at the end of epoch 30, iters 3720\n",
            "(epoch: 31, iters: 80, time: 0.272) G_GAN: 1.315 G_GAN_Feat: 2.448 G_VGG: 1.904 D_real: 0.338 D_fake: 0.284 \n",
            "End of epoch 31 / 200 \t Time Taken: 25 sec\n",
            "(epoch: 32, iters: 56, time: 0.204) G_GAN: 0.953 G_GAN_Feat: 2.544 G_VGG: 1.836 D_real: 0.362 D_fake: 0.295 \n",
            "End of epoch 32 / 200 \t Time Taken: 25 sec\n",
            "(epoch: 33, iters: 32, time: 0.204) G_GAN: 0.739 G_GAN_Feat: 2.433 G_VGG: 1.758 D_real: 0.302 D_fake: 0.379 \n",
            "saving the latest model (epoch 33, total_steps 4000)\n",
            "End of epoch 33 / 200 \t Time Taken: 28 sec\n",
            "(epoch: 34, iters: 8, time: 0.205) G_GAN: 1.100 G_GAN_Feat: 2.604 G_VGG: 2.231 D_real: 0.320 D_fake: 0.313 \n",
            "(epoch: 34, iters: 108, time: 0.201) G_GAN: 0.900 G_GAN_Feat: 2.615 G_VGG: 1.886 D_real: 0.263 D_fake: 0.337 \n",
            "End of epoch 34 / 200 \t Time Taken: 25 sec\n",
            "(epoch: 35, iters: 84, time: 0.204) G_GAN: 1.664 G_GAN_Feat: 3.305 G_VGG: 1.908 D_real: 0.434 D_fake: 0.299 \n",
            "End of epoch 35 / 200 \t Time Taken: 25 sec\n",
            "saving the model at the end of epoch 35, iters 4340\n",
            "(epoch: 36, iters: 60, time: 0.258) G_GAN: 0.425 G_GAN_Feat: 0.771 G_VGG: 1.749 D_real: 0.456 D_fake: 0.621 \n",
            "End of epoch 36 / 200 \t Time Taken: 25 sec\n",
            "(epoch: 37, iters: 36, time: 0.204) G_GAN: 1.086 G_GAN_Feat: 1.778 G_VGG: 1.743 D_real: 0.537 D_fake: 0.288 \n",
            "End of epoch 37 / 200 \t Time Taken: 25 sec\n",
            "(epoch: 38, iters: 12, time: 0.204) G_GAN: 0.476 G_GAN_Feat: 1.939 G_VGG: 1.796 D_real: 0.421 D_fake: 0.582 \n",
            "(epoch: 38, iters: 112, time: 0.201) G_GAN: 0.658 G_GAN_Feat: 2.159 G_VGG: 1.926 D_real: 0.926 D_fake: 0.687 \n",
            "End of epoch 38 / 200 \t Time Taken: 25 sec\n",
            "(epoch: 39, iters: 88, time: 0.204) G_GAN: 0.849 G_GAN_Feat: 2.837 G_VGG: 1.725 D_real: 0.238 D_fake: 0.366 \n",
            "End of epoch 39 / 200 \t Time Taken: 25 sec\n",
            "(epoch: 40, iters: 64, time: 0.204) G_GAN: 1.195 G_GAN_Feat: 2.825 G_VGG: 1.737 D_real: 0.412 D_fake: 0.356 \n",
            "End of epoch 40 / 200 \t Time Taken: 25 sec\n",
            "saving the model at the end of epoch 40, iters 4960\n",
            "(epoch: 41, iters: 40, time: 0.275) G_GAN: 1.689 G_GAN_Feat: 2.386 G_VGG: 1.742 D_real: 0.904 D_fake: 0.250 \n",
            "saving the latest model (epoch 41, total_steps 5000)\n",
            "End of epoch 41 / 200 \t Time Taken: 31 sec\n",
            "(epoch: 42, iters: 16, time: 0.205) G_GAN: 1.718 G_GAN_Feat: 3.475 G_VGG: 1.722 D_real: 0.259 D_fake: 0.301 \n",
            "(epoch: 42, iters: 116, time: 0.201) G_GAN: 0.818 G_GAN_Feat: 3.059 G_VGG: 1.691 D_real: 0.294 D_fake: 0.323 \n",
            "End of epoch 42 / 200 \t Time Taken: 25 sec\n",
            "(epoch: 43, iters: 92, time: 0.204) G_GAN: 1.460 G_GAN_Feat: 3.328 G_VGG: 1.730 D_real: 0.572 D_fake: 0.335 \n",
            "End of epoch 43 / 200 \t Time Taken: 25 sec\n",
            "(epoch: 44, iters: 68, time: 0.204) G_GAN: 0.835 G_GAN_Feat: 3.065 G_VGG: 1.838 D_real: 0.250 D_fake: 0.369 \n",
            "End of epoch 44 / 200 \t Time Taken: 25 sec\n",
            "(epoch: 45, iters: 44, time: 0.204) G_GAN: 0.417 G_GAN_Feat: 3.301 G_VGG: 1.731 D_real: 0.176 D_fake: 0.772 \n",
            "End of epoch 45 / 200 \t Time Taken: 25 sec\n",
            "saving the model at the end of epoch 45, iters 5580\n",
            "(epoch: 46, iters: 20, time: 0.259) G_GAN: 2.152 G_GAN_Feat: 4.241 G_VGG: 1.691 D_real: 1.218 D_fake: 0.369 \n",
            "(epoch: 46, iters: 120, time: 0.201) G_GAN: 0.807 G_GAN_Feat: 1.992 G_VGG: 1.666 D_real: 0.288 D_fake: 0.328 \n",
            "End of epoch 46 / 200 \t Time Taken: 25 sec\n",
            "(epoch: 47, iters: 96, time: 0.204) G_GAN: 0.656 G_GAN_Feat: 1.869 G_VGG: 1.522 D_real: 0.319 D_fake: 0.423 \n",
            "End of epoch 47 / 200 \t Time Taken: 25 sec\n",
            "(epoch: 48, iters: 72, time: 0.204) G_GAN: 1.309 G_GAN_Feat: 3.546 G_VGG: 1.650 D_real: 0.391 D_fake: 0.284 \n",
            "End of epoch 48 / 200 \t Time Taken: 25 sec\n",
            "(epoch: 49, iters: 48, time: 0.204) G_GAN: 0.685 G_GAN_Feat: 2.398 G_VGG: 1.602 D_real: 0.249 D_fake: 0.396 \n",
            "saving the latest model (epoch 49, total_steps 6000)\n",
            "End of epoch 49 / 200 \t Time Taken: 28 sec\n",
            "(epoch: 50, iters: 24, time: 0.205) G_GAN: 1.283 G_GAN_Feat: 3.623 G_VGG: 1.600 D_real: 0.336 D_fake: 0.330 \n",
            "(epoch: 50, iters: 124, time: 0.201) G_GAN: 1.480 G_GAN_Feat: 3.992 G_VGG: 1.616 D_real: 0.214 D_fake: 0.267 \n",
            "End of epoch 50 / 200 \t Time Taken: 25 sec\n",
            "saving the model at the end of epoch 50, iters 6200\n",
            "(epoch: 51, iters: 100, time: 0.201) G_GAN: 1.600 G_GAN_Feat: 5.330 G_VGG: 1.601 D_real: 0.410 D_fake: 0.193 \n",
            "End of epoch 51 / 200 \t Time Taken: 25 sec\n",
            "Traceback (most recent call last):\n",
            "  File \"train_video.py\", line 137, in <module>\n",
            "    loss_G.backward()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\", line 396, in backward\n",
            "    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\", line 175, in backward\n",
            "    allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GDg3CeW_1TD"
      },
      "source": [
        "### Continue Training\n",
        "To pick up from a previous training session run the same command you ran to start with and append `--continue_train` to the end of the command."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5q3dE9S_5eg"
      },
      "source": [
        "!python train_video.py --name aritra-ray --dataroot ./datasets/aritra-ray/ --save_epoch_freq 1 --continue_train "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jly_3OyBoGg2"
      },
      "source": [
        "#Generating Videos\n",
        "\n",
        "To generate a video from your completed model, run the `generate_video.py` script. I recommend keeping the `--name` and `--dataroot` arguments the same.\n",
        "\n",
        "There are a number of additional arguments you’ll need to include in this command:\n",
        "\n",
        "\n",
        "*   `--fps` frame rate for your video\n",
        "*   `--how_many` how many frames you want to create (this + fps = video length)\n",
        "*   `--which_epoch` which epoch you want to generate videos from (note: if you choose `133` but there’s no epoch 133 model file, this will throw an error)\n",
        "*   `--start_from` by default your video will start predicting images from the 60th frame of your training set. You can pass in the path to a different frame to have it start from that frame\n",
        "\n",
        "I recommend playing with both the `--which_epoch` and `--start_from` arguments as you can get dramatically different results by changing in the inputs here.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INVUtG-pt_F6",
        "outputId": "44afb043-a9d2-4bfe-c2f6-b17010b13544",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!python generate_video.py --name aritra-ray --dataroot ./datasets/aritra-ray/ --fps 24 --how_many 600 --which_epoch 50 --start_from 'video' "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------ Options -------------\n",
            "aspect_ratio: 1.0\n",
            "batchSize: 1\n",
            "checkpoints_dir: ./checkpoints\n",
            "cluster_path: features_clustered_010.npy\n",
            "data_type: 32\n",
            "dataroot: ./datasets/aritra-ray/\n",
            "display_winsize: 512\n",
            "engine: None\n",
            "export_onnx: None\n",
            "feat_num: 3\n",
            "fineSize: 512\n",
            "fp16: False\n",
            "fps: 24.0\n",
            "gpu_ids: [0]\n",
            "heat_seeking_lvl: 0\n",
            "how_many: 600\n",
            "input_nc: 3\n",
            "instance_feat: False\n",
            "isTrain: False\n",
            "label_feat: False\n",
            "label_nc: 35\n",
            "loadSize: 1024\n",
            "load_features: False\n",
            "local_rank: 0\n",
            "max_dataset_size: inf\n",
            "model: pix2pixHD\n",
            "nThreads: 2\n",
            "n_blocks_global: 9\n",
            "n_blocks_local: 3\n",
            "n_clusters: 10\n",
            "n_downsample_E: 4\n",
            "n_downsample_global: 4\n",
            "n_local_enhancers: 1\n",
            "name: aritra-ray\n",
            "nef: 16\n",
            "netG: global\n",
            "ngf: 64\n",
            "niter_fix_global: 0\n",
            "no_crop: True\n",
            "no_flip: False\n",
            "no_instance: False\n",
            "norm: instance\n",
            "ntest: inf\n",
            "onnx: None\n",
            "output_nc: 3\n",
            "phase: test\n",
            "png: False\n",
            "pstart: 1\n",
            "pstop: 1\n",
            "resize_or_crop: scale_width\n",
            "results_dir: ./results/\n",
            "scheduled_sampling: False\n",
            "serial_batches: False\n",
            "ss_recursion_prob: 0.2\n",
            "start_frames_before: 1\n",
            "start_from: video\n",
            "tf_log: False\n",
            "use_dropout: False\n",
            "use_encoded_image: False\n",
            "verbose: False\n",
            "video_mode: False\n",
            "which_epoch: 50\n",
            "zoom_cres: False\n",
            "zoom_inc: 24\n",
            "zoom_lvl: 0\n",
            "-------------- End ----------------\n",
            "CustomDatasetDataLoader\n",
            "dataset [FrameDataset] was created\n",
            "FrameDataset initialized from: ./datasets/aritra-ray/test_frames\n",
            "contains 60 frames, 59 consecutive pairs\n",
            "\n",
            "100% 600/600 [01:01<00:00,  9.82it/s]\n",
            "ffmpeg -v 16 -framerate 24 -i ./checkpoints/aritra-ray/frames/frame-%05d.jpg -ss 1 -q:v 2 -filter:v \"crop=1280:720:0:16\" ./checkpoints/aritra-ray/output/epoch-50_aritra-ray_25.0-s_24.0-fps.mp4\n",
            "building video from frames\n",
            "video ready:\n",
            "./checkpoints/aritra-ray/output/epoch-50_aritra-ray_25.0-s_24.0-fps.mp4\n"
          ]
        }
      ]
    }
  ]
}